"""
é€šç”¨ Log è§£æžå™¨

è™•ç†ä¸€èˆ¬çš„æ‡‰ç”¨ç¨‹å¼ log
"""

from typing import List, Dict, Any
import re
from .base_log_parser import BaseLogParser
from langchain.schema import Document


class GeneralLogParser(BaseLogParser):
    """é€šç”¨ log è§£æžå™¨ï¼ˆåŽŸ LogParser çš„å¯¦ç¾ï¼‰"""
    
    def get_log_type(self) -> str:
        return "general"
    
    def can_parse(self, file_path: str, content_sample: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºä¸€èˆ¬ log æ ¼å¼"""
        # å¦‚æžœä¸æ˜¯ç‰¹æ®Šæ ¼å¼ï¼Œå°±ç”¨é€šç”¨è§£æžå™¨
        patterns = self.get_patterns()
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¸¸è¦‹çš„ log å…ƒç´ 
        has_timestamp = bool(re.search(patterns['timestamp'], content_sample))
        has_log_level = bool(re.search(patterns['level'], content_sample))
        
        # å¦‚æžœæœ‰æ™‚é–“æˆ³æˆ–æ—¥èªŒç´šåˆ¥ï¼Œå°±èªç‚ºæ˜¯ä¸€èˆ¬ log
        return has_timestamp or has_log_level
    
    def get_patterns(self) -> Dict[str, str]:
        """è¿”å›žä¸€èˆ¬ log çš„æ¨¡å¼"""
        return {
            'timestamp': r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}',
            'iso_timestamp': r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',
            'level': r'\b(DEBUG|INFO|WARNING|WARN|ERROR|CRITICAL|FATAL|TRACE)\b',
            'ip': r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b',
            'error_keywords': r'(error|exception|failed|failure|critical|fatal|panic|crash)',
            'stack_trace': r'^\s+at\s+[\w.$]+\([\w.]+:\d+\)',  # Java style
            'python_trace': r'^\s+File\s+"[^"]+",\s+line\s+\d+',  # Python style
        }
    
    def analyze_log_structure(self, content: str) -> Dict[str, Any]:
        """åˆ†æžä¸€èˆ¬ log çš„çµæ§‹"""
        lines = content.split('\n')
        total_lines = len(lines)
        patterns = self.get_patterns()
        
        # æª¢æ¸¬æ™‚é–“æˆ³
        timestamp_count = len(re.findall(patterns['timestamp'], content))
        iso_timestamp_count = len(re.findall(patterns['iso_timestamp'], content))
        has_timestamps = (timestamp_count + iso_timestamp_count) > total_lines * 0.3
        
        # çµ±è¨ˆéŒ¯èª¤ç´šåˆ¥
        error_levels = re.findall(patterns['level'], content, re.IGNORECASE)
        level_counts = {}
        for level in error_levels:
            level = level.upper()
            level_counts[level] = level_counts.get(level, 0) + 1
        
        # çµ±è¨ˆéŒ¯èª¤é—œéµå­—
        error_keywords = re.findall(patterns['error_keywords'], content, re.IGNORECASE)
        
        # æª¢æ¸¬å †ç–Šè¿½è¹¤
        stack_traces = len(re.findall(patterns['stack_trace'], content, re.MULTILINE))
        python_traces = len(re.findall(patterns['python_trace'], content, re.MULTILINE))
        
        # æå–æ™‚é–“ç¯„åœ
        time_range = self.extract_time_range(content)
        
        return {
            'total_lines': total_lines,
            'has_timestamps': has_timestamps,
            'timestamp_format': 'iso' if iso_timestamp_count > timestamp_count else 'standard',
            'level_counts': level_counts,
            'error_count': len(error_keywords),
            'stack_trace_count': stack_traces + python_traces,
            'file_size_mb': len(content) / (1024 * 1024),
            'time_range': time_range,
            'severity_score': self._calculate_severity_score(level_counts, len(error_keywords)),
        }
    
    def _calculate_severity_score(self, level_counts: Dict[str, int], error_count: int) -> int:
        """è¨ˆç®—åš´é‡ç¨‹åº¦åˆ†æ•¸ï¼ˆ0-100ï¼‰"""
        score = 0
        
        # æ ¹æ“šéŒ¯èª¤ç´šåˆ¥è¨ˆåˆ†
        weights = {
            'FATAL': 20,
            'CRITICAL': 15,
            'ERROR': 10,
            'WARN': 5,
            'WARNING': 5,
        }
        
        for level, count in level_counts.items():
            score += weights.get(level, 0) * min(count, 10)  # æœ€å¤šè¨ˆç®— 10 å€‹
        
        # éŒ¯èª¤é—œéµå­—åŠ åˆ†
        score += min(error_count * 2, 30)
        
        return min(score, 100)
    
    def parse_content(self, content: str, file_path: str, log_info: Dict[str, Any]) -> List[Document]:
        """è§£æžä¸€èˆ¬ log å…§å®¹"""
        documents = []
        
        # æ ¹æ“šåš´é‡ç¨‹åº¦é¸æ“‡è§£æžç­–ç•¥
        severity_score = log_info.get('severity_score', 0)
        
        if severity_score > 50:
            # é«˜åš´é‡åº¦ï¼šæŒ‰éŒ¯èª¤åˆ†çµ„
            print(f"âš ï¸  æª¢æ¸¬åˆ°é«˜åš´é‡åº¦ log (åˆ†æ•¸: {severity_score})ï¼Œä½¿ç”¨éŒ¯èª¤åˆ†çµ„ç­–ç•¥")
            documents = self._parse_by_errors(content, file_path, log_info)
        elif log_info.get('has_timestamps', False):
            # æœ‰æ™‚é–“æˆ³ï¼šæŒ‰æ™‚é–“åˆ†çµ„
            print("ðŸ“… ä½¿ç”¨æ™‚é–“åˆ†çµ„ç­–ç•¥")
            documents = self._parse_by_time_blocks(content, file_path, log_info)
        else:
            # æ¨™æº–åˆ†å‰²
            print("ðŸ“„ ä½¿ç”¨æ¨™æº–åˆ†å‰²ç­–ç•¥")
            documents = self._standard_parse(content, file_path, log_info)
        
        return documents
    
    def _parse_by_errors(self, content: str, file_path: str, log_info: Dict[str, Any]) -> List[Document]:
        """æŒ‰éŒ¯èª¤åˆ†çµ„è§£æž"""
        documents = []
        patterns = self.get_patterns()
        
        # æ‰¾å‡ºæ‰€æœ‰éŒ¯èª¤ä½ç½®
        error_positions = []
        for match in re.finditer(patterns['error_keywords'], content, re.IGNORECASE | re.MULTILINE):
            error_positions.append({
                'pos': match.start(),
                'type': match.group(),
                'line': content[:match.start()].count('\n')
            })
        
        # ç‚ºæ¯å€‹éŒ¯èª¤å‰µå»ºä¸Šä¸‹æ–‡æ–‡æª”
        for i, error_info in enumerate(error_positions):
            # æ‰¾åˆ°éŒ¯èª¤å‰å¾Œçš„é‚Šç•Œ
            start_pos = max(0, error_info['pos'] - 1000)
            end_pos = min(len(content), error_info['pos'] + 2000)
            
            # èª¿æ•´åˆ°è¡Œé‚Šç•Œ
            if start_pos > 0:
                start_pos = content.rfind('\n', 0, start_pos) + 1
            if end_pos < len(content):
                end_pos = content.find('\n', end_pos)
                if end_pos == -1:
                    end_pos = len(content)
            
            error_context = content[start_pos:end_pos]
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å †ç–Šè¿½è¹¤
            has_stack_trace = bool(re.search(patterns['stack_trace'], error_context)) or \
                            bool(re.search(patterns['python_trace'], error_context))
            
            doc = self.create_document(
                error_context,
                file_path,
                {
                    'chunk_method': 'error_context',
                    'error_index': i,
                    'error_type': error_info['type'],
                    'error_line': error_info['line'],
                    'has_stack_trace': has_stack_trace,
                    'context_type': 'error'
                }
            )
            documents.append(doc)
        
        return documents
    
    def _parse_by_time_blocks(self, content: str, file_path: str, log_info: Dict[str, Any]) -> List[Document]:
        """æŒ‰æ™‚é–“å¡Šåˆ†å‰²"""
        lines = content.split('\n')
        documents = []
        current_block = []
        current_size = 0
        block_start_time = None
        patterns = self.get_patterns()
        
        for line in lines:
            # æª¢æŸ¥æ™‚é–“æˆ³
            timestamp_match = re.search(patterns['timestamp'], line) or \
                            re.search(patterns['iso_timestamp'], line)
            
            if timestamp_match and not block_start_time:
                block_start_time = timestamp_match.group()
            
            current_block.append(line)
            current_size += len(line) + 1
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦å‰µå»ºæ–°å¡Š
            if current_size >= self.chunk_size and timestamp_match:
                block_content = '\n'.join(current_block)
                
                # åˆ†æžæ­¤å¡Šçš„å…§å®¹
                block_error_count = len(re.findall(patterns['error_keywords'], 
                                                  block_content, re.IGNORECASE))
                
                doc = self.create_document(
                    block_content,
                    file_path,
                    {
                        'chunk_method': 'time_block',
                        'block_start_time': block_start_time,
                        'block_error_count': block_error_count,
                        'context_type': 'temporal'
                    }
                )
                documents.append(doc)
                
                # ä¿ç•™é‡ç–Š
                overlap_lines = max(1, self.chunk_overlap // 50)
                current_block = current_block[-overlap_lines:]
                current_size = sum(len(line) + 1 for line in current_block)
                block_start_time = None
        
        # è™•ç†å‰©é¤˜å…§å®¹
        if current_block:
            block_content = '\n'.join(current_block)
            doc = self.create_document(
                block_content,
                file_path,
                {
                    'chunk_method': 'time_block_final',
                    'block_start_time': block_start_time,
                    'context_type': 'temporal'
                }
            )
            documents.append(doc)
        
        return documents
    
    def _standard_parse(self, content: str, file_path: str, log_info: Dict[str, Any]) -> List[Document]:
        """æ¨™æº–åˆ†å‰²"""
        chunks = self.smart_split(content)
        documents = []
        
        for i, chunk in enumerate(chunks):
            doc = self.create_document(
                chunk,
                file_path,
                {
                    'chunk_method': 'standard',
                    'chunk_index': i,
                    'context_type': 'general'
                }
            )
            documents.append(doc)
        
        return documents