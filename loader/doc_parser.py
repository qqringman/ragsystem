import os
from pathlib import Path

from langchain_community.document_loaders import (
    PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader,
    UnstructuredMarkdownLoader, UnstructuredHTMLLoader, JSONLoader, TextLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config import get_config

# å°å…¥ log è§£æå™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from .log_parser import LogParser
    log_parser = LogParser()
except ImportError:
    log_parser = None
    print("âš ï¸  Log è§£æå™¨æœªå®‰è£ï¼Œå°‡ä½¿ç”¨æ¨™æº–æ–‡å­—è™•ç†")


def load_and_split_documents(file_paths):
    """
    è¼‰å…¥ä¸¦åˆ†å‰²æ–‡ä»¶
    
    Args:
        file_paths: æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        
    Returns:
        åˆ†å‰²å¾Œçš„æ–‡ä»¶åˆ—è¡¨
    """
    docs = []
    
    for path in file_paths:
        if not os.path.exists(path):
            print(f"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {path}")
            continue
            
        ext = Path(path).suffix.lower()
        file_size_mb = os.path.getsize(path) / (1024 * 1024)
        
        print(f"ğŸ“„ è™•ç†æª”æ¡ˆ: {Path(path).name} ({file_size_mb:.2f} MB)")
        
        try:
            # ç‰¹æ®Šè™•ç† log æª”æ¡ˆ
            if ext == ".log" or (ext == ".txt" and "log" in Path(path).stem.lower()):
                if log_parser:
                    print(f"ğŸ“Š ä½¿ç”¨å°ˆé–€çš„ Log è§£æå™¨è™•ç†...")
                    loaded_docs = log_parser.parse_log_file(path)
                    
                    # å¦‚æœæ˜¯å¤§å‹ log æª”æ¡ˆï¼Œé¡¯ç¤ºåˆ†æçµæœ
                    if file_size_mb > 1:
                        print(f"   âœ… Log æª”æ¡ˆåˆ†æå®Œæˆï¼š{len(loaded_docs)} å€‹ç‰‡æ®µ")
                        if loaded_docs and 'error_count' in loaded_docs[0].metadata:
                            total_errors = sum(doc.metadata.get('error_count', 0) for doc in loaded_docs)
                            print(f"   ğŸ” ç™¼ç¾ {total_errors} å€‹éŒ¯èª¤ç›¸é—œæ¢ç›®")
                else:
                    # é™ç´šåˆ°æ–‡å­—è¼‰å…¥å™¨
                    loader = TextLoader(path, encoding='utf-8')
                    loaded_docs = loader.load()
                    
            elif ext == ".pdf":
                loader = PyPDFLoader(path)
                loaded_docs = loader.load()
                
            elif ext in [".doc", ".docx"]:
                loader = UnstructuredWordDocumentLoader(path)
                loaded_docs = loader.load()
                
            elif ext in [".xls", ".xlsx"]:
                loader = UnstructuredExcelLoader(path)
                loaded_docs = loader.load()
                
            elif ext == ".md":
                loader = UnstructuredMarkdownLoader(path)
                loaded_docs = loader.load()
                
            elif ext in [".html", ".htm"]:
                loader = UnstructuredHTMLLoader(path)
                loaded_docs = loader.load()
                
            elif ext == ".json":
                loader = JSONLoader(path, jq_schema=".", text_content=False)
                loaded_docs = loader.load()
                
            elif ext in [".txt", ".csv"]:
                # ä¸€èˆ¬æ–‡å­—æª”æ¡ˆ
                loader = TextLoader(path, encoding='utf-8')
                loaded_docs = loader.load()
                
            else:
                print(f"âš ï¸  ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {ext}")
                continue

            # ç‚ºæ‰€æœ‰æ–‡æª”æ·»åŠ æª”æ¡ˆå¤§å°å…ƒæ•¸æ“š
            for doc in loaded_docs:
                doc.metadata['file_size_mb'] = file_size_mb
                doc.metadata['file_type'] = ext[1:]  # ç§»é™¤é»è™Ÿ
                
            docs.extend(loaded_docs)
            print(f"âœ… æˆåŠŸè¼‰å…¥: {Path(path).name} ({len(loaded_docs)} å€‹æ–‡ä»¶)")
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥æª”æ¡ˆ {path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            continue

    if not docs:
        print("âš ï¸  æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æ–‡ä»¶")
        return []

    # æª¢æŸ¥æ˜¯å¦éœ€è¦é€²ä¸€æ­¥åˆ†å‰²
    # å°æ–¼å·²ç¶“ç”± LogParser è™•ç†çš„æ–‡æª”ï¼Œä¸å†åˆ†å‰²
    already_split_docs = [doc for doc in docs if doc.metadata.get('chunk_method')]
    need_split_docs = [doc for doc in docs if not doc.metadata.get('chunk_method')]
    
    if need_split_docs:
        # ä½¿ç”¨é…ç½®ä¸­çš„ chunk_size å’Œ chunk_overlap
        chunk_size = int(get_config("CHUNK_SIZE", "1000"))
        chunk_overlap = int(get_config("CHUNK_OVERLAP", "100"))
        
        print(f"ğŸ“„ é–‹å§‹åˆ†å‰² {len(need_split_docs)} å€‹æ–‡ä»¶ (chunk_size={chunk_size}, overlap={chunk_overlap})...")
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, 
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
        split_docs = splitter.split_documents(need_split_docs)
        print(f"âœ… æ–‡ä»¶åˆ†å‰²å®Œæˆï¼Œæ–°å¢ {len(split_docs)} å€‹ç‰‡æ®µ")
        
        # åˆä½µå·²åˆ†å‰²å’Œæ–°åˆ†å‰²çš„æ–‡æª”
        all_docs = already_split_docs + split_docs
    else:
        all_docs = already_split_docs
    
    print(f"ğŸ“š ç¸½å…±è™•ç†å®Œæˆ {len(all_docs)} å€‹æ–‡æª”ç‰‡æ®µ")
    
    return all_docs