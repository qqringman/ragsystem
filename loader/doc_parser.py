import os
from pathlib import Path

from langchain_community.document_loaders import (
    PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader,
    UnstructuredMarkdownLoader, UnstructuredHTMLLoader, JSONLoader, TextLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config import get_config

# å°Žå…¥ log è§£æžå™¨ç®¡ç†å™¨
try:
    from .log_parser_manager import log_parser_manager
    HAS_LOG_PARSER = True
except ImportError:
    log_parser_manager = None
    HAS_LOG_PARSER = False
    print("âš ï¸  Log è§£æžå™¨æœªå®‰è£ï¼Œå°‡ä½¿ç”¨æ¨™æº–æ–‡å­—è™•ç†")


def load_and_split_documents(file_paths):
    """
    è¼‰å…¥ä¸¦åˆ†å‰²æ–‡ä»¶
    
    Args:
        file_paths: æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        
    Returns:
        åˆ†å‰²å¾Œçš„æ–‡ä»¶åˆ—è¡¨
    """
    docs = []
    
    for path in file_paths:
        if not os.path.exists(path):
            print(f"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {path}")
            continue
            
        ext = Path(path).suffix.lower()
        file_size_mb = os.path.getsize(path) / (1024 * 1024)
        
        print(f"ðŸ“„ è™•ç†æª”æ¡ˆ: {Path(path).name} ({file_size_mb:.2f} MB)")
        
        try:
            # ç‰¹æ®Šè™•ç† log æª”æ¡ˆ
            if ext == ".log" or (ext == ".txt" and "log" in Path(path).stem.lower()):
                if HAS_LOG_PARSER and log_parser_manager:
                    print(f"ðŸ“Š ä½¿ç”¨å°ˆé–€çš„ Log è§£æžå™¨è™•ç†...")
                    loaded_docs = log_parser_manager.parse_log_file(path)
                    
                    # å¦‚æžœæ˜¯å¤§åž‹ log æª”æ¡ˆï¼Œé¡¯ç¤ºåˆ†æžçµæžœ
                    if file_size_mb > 1:
                        print(f"   âœ… Log æª”æ¡ˆåˆ†æžå®Œæˆï¼š{len(loaded_docs)} å€‹ç‰‡æ®µ")
                        
                        # é¡¯ç¤ºåˆ†æžçµ±è¨ˆ
                        if loaded_docs:
                            log_types = set(doc.metadata.get('log_type', 'unknown') for doc in loaded_docs)
                            print(f"   ðŸ“‹ Log é¡žåž‹: {', '.join(log_types)}")
                            
                            # å¦‚æžœæœ‰éŒ¯èª¤çµ±è¨ˆ
                            error_docs = [doc for doc in loaded_docs if doc.metadata.get('error_count', 0) > 0]
                            if error_docs:
                                total_errors = sum(doc.metadata.get('error_count', 0) for doc in error_docs)
                                print(f"   ðŸ” ç™¼ç¾ {total_errors} å€‹éŒ¯èª¤ç›¸é—œæ¢ç›®")
                            
                            # å¦‚æžœæœ‰å´©æ½°è³‡è¨Š
                            crash_docs = [doc for doc in loaded_docs if doc.metadata.get('crash_type')]
                            if crash_docs:
                                crash_types = set(doc.metadata.get('crash_type', 'unknown') for doc in crash_docs)
                                print(f"   ðŸ’¥ å´©æ½°é¡žåž‹: {', '.join(crash_types)}")
                else:
                    # é™ç´šåˆ°æ–‡å­—è¼‰å…¥å™¨
                    loader = TextLoader(path, encoding='utf-8')
                    loaded_docs = loader.load()
                    
            elif ext == ".pdf":
                loader = PyPDFLoader(path)
                loaded_docs = loader.load()
                
            elif ext in [".doc", ".docx"]:
                loader = UnstructuredWordDocumentLoader(path)
                loaded_docs = loader.load()
                
            elif ext in [".xls", ".xlsx"]:
                loader = UnstructuredExcelLoader(path)
                loaded_docs = loader.load()
                
            elif ext == ".md":
                loader = UnstructuredMarkdownLoader(path)
                loaded_docs = loader.load()
                
            elif ext in [".html", ".htm"]:
                loader = UnstructuredHTMLLoader(path)
                loaded_docs = loader.load()
                
            elif ext == ".json":
                loader = JSONLoader(path, jq_schema=".", text_content=False)
                loaded_docs = loader.load()
                
            elif ext in [".txt", ".csv"]:
                # ä¸€èˆ¬æ–‡å­—æª”æ¡ˆ
                loader = TextLoader(path, encoding='utf-8')
                loaded_docs = loader.load()
                
            else:
                print(f"âš ï¸  ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {ext}")
                continue

            # ç‚ºæ‰€æœ‰æ–‡æª”æ·»åŠ æª”æ¡ˆå¤§å°å…ƒæ•¸æ“š
            for doc in loaded_docs:
                doc.metadata['file_size_mb'] = file_size_mb
                doc.metadata['file_type'] = ext[1:]  # ç§»é™¤é»žè™Ÿ
                
            docs.extend(loaded_docs)
            print(f"âœ… æˆåŠŸè¼‰å…¥: {Path(path).name} ({len(loaded_docs)} å€‹æ–‡ä»¶)")
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥æª”æ¡ˆ {path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            continue

    if not docs:
        print("âš ï¸  æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æ–‡ä»¶")
        return []

    # æª¢æŸ¥æ˜¯å¦éœ€è¦é€²ä¸€æ­¥åˆ†å‰²
    # å°æ–¼å·²ç¶“ç”± LogParser è™•ç†çš„æ–‡æª”ï¼Œä¸å†åˆ†å‰²
    already_split_docs = [doc for doc in docs if doc.metadata.get('chunk_method')]
    need_split_docs = [doc for doc in docs if not doc.metadata.get('chunk_method')]
    
    if need_split_docs:
        # ä½¿ç”¨é…ç½®ä¸­çš„ chunk_size å’Œ chunk_overlap
        chunk_size = int(get_config("CHUNK_SIZE", "1000"))
        chunk_overlap = int(get_config("CHUNK_OVERLAP", "100"))
        
        print(f"ðŸ“„ é–‹å§‹åˆ†å‰² {len(need_split_docs)} å€‹æ–‡ä»¶ (chunk_size={chunk_size}, overlap={chunk_overlap})...")
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, 
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
        split_docs = splitter.split_documents(need_split_docs)
        print(f"âœ… æ–‡ä»¶åˆ†å‰²å®Œæˆï¼Œæ–°å¢ž {len(split_docs)} å€‹ç‰‡æ®µ")
        
        # åˆä½µå·²åˆ†å‰²å’Œæ–°åˆ†å‰²çš„æ–‡æª”
        all_docs = already_split_docs + split_docs
    else:
        all_docs = already_split_docs
    
    print(f"ðŸ“š ç¸½å…±è™•ç†å®Œæˆ {len(all_docs)} å€‹æ–‡æª”ç‰‡æ®µ")
    
    # é¡¯ç¤ºè™•ç†çµ±è¨ˆ
    if all_docs:
        # çµ±è¨ˆä¸åŒé¡žåž‹çš„æ–‡æª”
        doc_types = {}
        for doc in all_docs:
            doc_type = doc.metadata.get('file_type', 'unknown')
            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
        
        if len(doc_types) > 1 or 'log' in doc_types:
            print(f"ðŸ“Š æ–‡æª”é¡žåž‹çµ±è¨ˆ:")
            for doc_type, count in doc_types.items():
                print(f"   - {doc_type}: {count} å€‹ç‰‡æ®µ")
    
    return all_docs