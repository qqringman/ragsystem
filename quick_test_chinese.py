#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæ¸¬è©¦ä¸­æ–‡åµŒå…¥æ¨¡å‹
ç›´æ¥é‹è¡Œ: python quick_test_chinese.py
"""

import sys
import time
from sentence_transformers import SentenceTransformer

def test_chinese_embedding():
    """æ¸¬è©¦ä¸­æ–‡åµŒå…¥æ¨¡å‹"""
    print("="*50)
    print("ğŸ‡¨ğŸ‡³ ä¸­æ–‡åµŒå…¥æ¨¡å‹æ¸¬è©¦")
    print("="*50)
    
    # 1. è¼‰å…¥æ¨¡å‹
    print("\n1ï¸âƒ£ æ­£åœ¨è¼‰å…¥æ¨¡å‹ shibing624/text2vec-base-chinese...")
    print("   é¦–æ¬¡ä½¿ç”¨æœƒè‡ªå‹•ä¸‹è¼‰ï¼ˆç´„ 400MBï¼‰ï¼Œè«‹è€å¿ƒç­‰å¾…...")
    
    start_time = time.time()
    try:
        model = SentenceTransformer('shibing624/text2vec-base-chinese')
        load_time = time.time() - start_time
        print(f"   âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼è€—æ™‚: {load_time:.2f} ç§’")
    except Exception as e:
        print(f"   âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
        print("\n   å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
        print("   1. æª¢æŸ¥ç¶²è·¯é€£æ¥")
        print("   2. ä½¿ç”¨ä»£ç†æˆ–é¡åƒç«™")
        print("   3. æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹")
        return False
    
    # 2. æ¸¬è©¦æ–‡æœ¬åµŒå…¥
    print("\n2ï¸âƒ£ æ¸¬è©¦ä¸­æ–‡æ–‡æœ¬åµŒå…¥...")
    test_sentences = [
        "äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼",
        "æ©Ÿå™¨å­¸ç¿’æ˜¯å¯¦ç¾äººå·¥æ™ºæ…§çš„é‡è¦æŠ€è¡“",
        "æ·±åº¦å­¸ç¿’ä½¿ç”¨ç¥ç¶“ç¶²è·¯ä¾†è™•ç†è¤‡é›œå•é¡Œ",
        "è‡ªç„¶èªè¨€è™•ç†è®“é›»è…¦èƒ½å¤ ç†è§£äººé¡èªè¨€",
        "ä»Šå¤©çš„å¤©æ°£éå¸¸æ™´æœ—",
        "æˆ‘å–œæ­¡åƒä¸­å¼æ–™ç†"
    ]
    
    start_time = time.time()
    embeddings = model.encode(test_sentences)
    encode_time = time.time() - start_time
    
    print(f"   âœ… æˆåŠŸç·¨ç¢¼ {len(test_sentences)} å€‹å¥å­")
    print(f"   â±ï¸  ç·¨ç¢¼è€—æ™‚: {encode_time:.2f} ç§’")
    print(f"   ğŸ“Š å‘é‡ç¶­åº¦: {embeddings.shape}")
    
    # 3. æ¸¬è©¦èªç¾©ç›¸ä¼¼åº¦
    print("\n3ï¸âƒ£ æ¸¬è©¦èªç¾©ç›¸ä¼¼åº¦æœå°‹...")
    query = "AI å’Œæ©Ÿå™¨å­¸ç¿’çš„é—œä¿‚"
    print(f"   æŸ¥è©¢: '{query}'")
    
    query_embedding = model.encode([query])[0]
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    from sklearn.metrics.pairwise import cosine_similarity
    similarities = cosine_similarity([query_embedding], embeddings)[0]
    
    # æ’åºä¸¦é¡¯ç¤ºçµæœ
    print("\n   ç›¸ä¼¼åº¦æ’å:")
    sorted_idx = similarities.argsort()[::-1]
    for i, idx in enumerate(sorted_idx):
        print(f"   {i+1}. {test_sentences[idx]}")
        print(f"      ç›¸ä¼¼åº¦: {similarities[idx]:.4f}")
    
    # 4. æ¸¬è©¦ç¹ç°¡è½‰æ›
    print("\n4ï¸âƒ£ æ¸¬è©¦ç¹ç°¡é«”æ··åˆè™•ç†...")
    mixed_texts = [
        "æ©Ÿå™¨å­¸ç¿’å¹«åŠ©æˆ‘å€‘è§£æ±ºå•é¡Œ",  # ç°¡é«”
        "æ©Ÿå™¨å­¸ç¿’å¹«åŠ©æˆ‘å€‘è§£æ±ºå•é¡Œ",  # ç¹é«”
        "äººå·¥æ™ºèƒ½ã¨æ©Ÿæ¢°å­¦ç¿’",        # æ··åˆæ—¥æ–‡
    ]
    
    try:
        mixed_embeddings = model.encode(mixed_texts)
        print("   âœ… æˆåŠŸè™•ç†ç¹ç°¡é«”æ··åˆæ–‡æœ¬")
        
        # æª¢æŸ¥ç¹ç°¡é«”çš„èªç¾©ç›¸ä¼¼åº¦
        sim = cosine_similarity([mixed_embeddings[0]], [mixed_embeddings[1]])[0][0]
        print(f"   ğŸ“Š ç¹ç°¡é«”ç›¸ä¼¼åº¦: {sim:.4f} (æ‡‰è©²æ¥è¿‘ 1.0)")
    except Exception as e:
        print(f"   âš ï¸  æ··åˆæ–‡æœ¬è™•ç†è­¦å‘Š: {str(e)}")
    
    # 5. æ•ˆèƒ½çµ±è¨ˆ
    print("\n5ï¸âƒ£ æ•ˆèƒ½çµ±è¨ˆ:")
    print(f"   - æ¨¡å‹è¼‰å…¥æ™‚é–“: {load_time:.2f} ç§’")
    print(f"   - å¹³å‡ç·¨ç¢¼é€Ÿåº¦: {len(test_sentences)/encode_time:.2f} å¥/ç§’")
    print(f"   - å‘é‡ç¶­åº¦: {embeddings.shape[1]}")
    
    print("\n" + "="*50)
    print("âœ… æ¸¬è©¦å®Œæˆï¼ä¸­æ–‡åµŒå…¥æ¨¡å‹é‹ä½œæ­£å¸¸")
    print("="*50)
    
    return True


def test_rag_integration():
    """æ¸¬è©¦èˆ‡ RAG ç³»çµ±çš„æ•´åˆ"""
    print("\n\nğŸ”— æ¸¬è©¦ RAG ç³»çµ±æ•´åˆ...")
    
    try:
        # è¨­å®šç’°å¢ƒè®Šæ•¸
        import os
        os.environ['EMBED_PROVIDER'] = 'huggingface'
        os.environ['HUGGINGFACE_MODEL'] = 'shibing624/text2vec-base-chinese'
        
        # æ¸¬è©¦è¼‰å…¥
        from vectorstore.index_manager import get_embeddings
        embeddings = get_embeddings()
        
        # æ¸¬è©¦åµŒå…¥åŠŸèƒ½
        test_text = "æª¢ç´¢å¢å¼·ç”ŸæˆæŠ€è¡“"
        vector = embeddings.embed_query(test_text)
        
        print(f"âœ… RAG æ•´åˆæ¸¬è©¦æˆåŠŸï¼")
        print(f"   - æˆåŠŸè¼‰å…¥åµŒå…¥æ¨¡å‹")
        print(f"   - æˆåŠŸç”Ÿæˆå‘é‡ (ç¶­åº¦: {len(vector)})")
        
    except ImportError:
        print("âš ï¸  ç„¡æ³•å°å…¥ RAG æ¨¡çµ„ï¼Œè«‹ç¢ºä¿åœ¨å°ˆæ¡ˆç›®éŒ„ä¸­é‹è¡Œ")
    except Exception as e:
        print(f"âŒ RAG æ•´åˆæ¸¬è©¦å¤±æ•—: {str(e)}")


if __name__ == "__main__":
    # æª¢æŸ¥ä¾è³´
    try:
        import sentence_transformers
        import sklearn
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„å¥—ä»¶: {e}")
        print("è«‹å…ˆå®‰è£: pip install sentence-transformers scikit-learn")
        sys.exit(1)
    
    # åŸ·è¡Œæ¸¬è©¦
    success = test_chinese_embedding()
    
    if success:
        test_rag_integration()
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("1. åœ¨ .env ä¸­è¨­å®š: HUGGINGFACE_MODEL=shibing624/text2vec-base-chinese")
        print("2. é‹è¡Œ RAG ç³»çµ±: streamlit run app.py")
        print("3. ä¸Šå‚³ä¸­æ–‡æ–‡æª”é€²è¡Œæ¸¬è©¦")