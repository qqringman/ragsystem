# -*- coding: utf-8 -*-
import os
from pathlib import Path
import streamlit as st
from rag_chain import run_rag
import tempfile
import sys
import json
from datetime import datetime

# é…ç½®ç³»çµ±æœƒè‡ªå‹•è¼‰å…¥ .env
from config import get_config, validate_config

# é©—è­‰é…ç½®
try:
    validate_config()
except ValueError as e:
    st.error(f"é…ç½®éŒ¯èª¤: {e}")
    st.stop()

print("ğŸ” æ­£åœ¨åŸ·è¡Œçš„ Python ç‰ˆæœ¬:", sys.executable)

st.set_page_config(page_title="RAG å…¨åŠŸèƒ½ç³»çµ±", layout="wide")
st.title("ğŸ§  RAG å•ç­”å¼•æ“")

# åˆå§‹åŒ– session state
if 'indexed_files' not in st.session_state:
    st.session_state.indexed_files = []
if 'vector_db_initialized' not in st.session_state:
    st.session_state.vector_db_initialized = False

# å´é‚Šæ¬„ - å‘é‡è³‡æ–™åº«ç®¡ç†
with st.sidebar:
    st.markdown("### ğŸ“š å‘é‡è³‡æ–™åº«ç‹€æ…‹")
    
    # æª¢æŸ¥å·²ç´¢å¼•çš„æª”æ¡ˆ
    index_file = "vector_db/indexed_files.json"
    if os.path.exists(index_file):
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                indexed_data = json.load(f)
                st.session_state.indexed_files = indexed_data.get('files', [])
        except:
            st.session_state.indexed_files = []
    
    if st.session_state.indexed_files:
        st.success(f"å·²ç´¢å¼• {len(st.session_state.indexed_files)} å€‹æª”æ¡ˆ")
        with st.expander("æŸ¥çœ‹å·²ç´¢å¼•æª”æ¡ˆ"):
            for file_info in st.session_state.indexed_files:
                st.write(f"ğŸ“„ {file_info['name']} ({file_info['date']})")
    else:
        st.info("å‘é‡è³‡æ–™åº«ç‚ºç©º")
    
    # æ¸…ç†å‘é‡è³‡æ–™åº«æŒ‰éˆ•
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå‘é‡è³‡æ–™åº«"):
        import shutil
        chroma_path = get_config("CHROMA_PERSIST_DIR", "vector_db/chroma")
        if os.path.exists(chroma_path):
            shutil.rmtree(chroma_path)
            os.makedirs(chroma_path, exist_ok=True)
        if os.path.exists(index_file):
            os.remove(index_file)
        st.session_state.indexed_files = []
        st.rerun()
    
    st.markdown("---")
    
    # ç³»çµ±è³‡è¨Š
    st.markdown("### ğŸ”§ ç³»çµ±è³‡è¨Š")
    st.markdown(f"Python ç‰ˆæœ¬: {sys.version.split()[0]}")
    
    # é¡¯ç¤ºç’°å¢ƒè®Šæ•¸ç‹€æ…‹
    st.markdown("### ğŸ”‘ API è¨­å®šç‹€æ…‹")
    llm_provider = os.getenv("LLM_PROVIDER", "æœªè¨­å®š")
    st.markdown(f"LLM Provider: `{llm_provider}`")
    
    # æª¢æŸ¥ API é‡‘é‘°
    if llm_provider == "openai":
        api_key_set = bool(os.getenv("OPENAI_API_KEY"))
        st.markdown(f"OpenAI API Key: {'âœ… å·²è¨­å®š' if api_key_set else 'âŒ æœªè¨­å®š'}")
    elif llm_provider in ["claude", "anthropic"]:
        api_key_set = bool(os.getenv("ANTHROPIC_API_KEY"))
        st.markdown(f"Anthropic API Key: {'âœ… å·²è¨­å®š' if api_key_set else 'âŒ æœªè¨­å®š'}")

# ä¸»è¦å…§å®¹å€åŸŸ
col1, col2 = st.columns([2, 1])

with col1:
    # è¼¸å…¥æŸ¥è©¢
    query = st.text_input("è«‹è¼¸å…¥ä½ çš„å•é¡Œï¼š", placeholder="ä¾‹å¦‚ï¼šåˆ†ææœ€è¿‘çš„éŒ¯èª¤æ—¥èªŒ")

with col2:
    # é¸æ“‡è³‡æ–™ä¾†æº
    data_source = st.multiselect("è«‹é¸æ“‡è³‡æ–™ä¾†æº", ["docs", "db"], default=["docs"])

# æª”æ¡ˆä¸Šå‚³å€åŸŸ
st.markdown("### ğŸ“¤ ä¸Šå‚³æ–°æª”æ¡ˆï¼ˆå¯é¸ï¼‰")
uploaded_files = st.file_uploader(
    "ä¸Šå‚³æ–‡ä»¶åˆ°å‘é‡è³‡æ–™åº«ï¼ˆæ”¯æ´ .pdf, .docx, .xlsx, .log, .txt ç­‰ï¼‰", 
    type=["pdf", "docx", "doc", "txt", "md", "html", "htm", "xlsx", "xls", "json", "log", "csv"], 
    accept_multiple_files=True,
    help="ä¸Šå‚³çš„æª”æ¡ˆæœƒè¢«æ°¸ä¹…åŠ å…¥å‘é‡è³‡æ–™åº«ï¼Œä¹‹å¾ŒæŸ¥è©¢æ™‚ä¸éœ€è¦é‡æ–°ä¸Šå‚³"
)

# è™•ç†ä¸Šå‚³çš„æª”æ¡ˆ
if uploaded_files:
    with st.spinner("æ­£åœ¨è™•ç†ä¸¦ç´¢å¼•æª”æ¡ˆ..."):
        temp_files = []
        temp_dir = tempfile.mkdtemp()
        
        for uploaded_file in uploaded_files:
            # å°‡ä¸Šå‚³çš„æª”æ¡ˆä¿å­˜åˆ°è‡¨æ™‚ç›®éŒ„
            temp_path = os.path.join(temp_dir, uploaded_file.name)
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            temp_files.append(temp_path)
        
        # ç´¢å¼•æª”æ¡ˆåˆ°å‘é‡è³‡æ–™åº«
        try:
            from loader.doc_parser import load_and_split_documents
            from vectorstore.index_manager import get_vectorstore
            
            # è¼‰å…¥å’Œåˆ†å‰²æ–‡ä»¶
            docs = load_and_split_documents(temp_files)
            
            if docs:
                # åŠ å…¥åˆ°å‘é‡è³‡æ–™åº«
                vs = get_vectorstore()
                vs.add_documents(docs)
                
                # æ›´æ–°å·²ç´¢å¼•æª”æ¡ˆåˆ—è¡¨
                new_files = []
                for uploaded_file in uploaded_files:
                    file_info = {
                        'name': uploaded_file.name,
                        'size': uploaded_file.size,
                        'date': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    new_files.append(file_info)
                    st.session_state.indexed_files.append(file_info)
                
                # ä¿å­˜ç´¢å¼•è¨˜éŒ„
                os.makedirs("vector_db", exist_ok=True)
                with open(index_file, 'w', encoding='utf-8') as f:
                    json.dump({'files': st.session_state.indexed_files}, f, ensure_ascii=False, indent=2)
                
                st.success(f"âœ… æˆåŠŸç´¢å¼• {len(uploaded_files)} å€‹æª”æ¡ˆåˆ°å‘é‡è³‡æ–™åº«")
                st.info("é€™äº›æª”æ¡ˆå·²æ°¸ä¹…ä¿å­˜ï¼Œä¹‹å¾Œçš„æŸ¥è©¢æœƒè‡ªå‹•åŒ…å«é€™äº›å…§å®¹")
            else:
                st.warning("ç„¡æ³•è¼‰å…¥æª”æ¡ˆå…§å®¹")
                
        except Exception as e:
            st.error(f"ç´¢å¼•æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        finally:
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            for temp_file in temp_files:
                try:
                    os.remove(temp_file)
                except:
                    pass
            try:
                os.rmdir(temp_dir)
            except:
                pass

# æŸ¥è©¢æŒ‰éˆ•
if st.button("ğŸ” æŸ¥è©¢", type="primary", use_container_width=True):
    if not query:
        st.warning("è«‹è¼¸å…¥æŸ¥è©¢å•é¡Œ")
    else:
        # æª¢æŸ¥æ˜¯å¦æœ‰å·²ç´¢å¼•çš„æª”æ¡ˆæˆ–æ–°ä¸Šå‚³çš„æª”æ¡ˆ
        if not st.session_state.indexed_files and "docs" in data_source:
            st.warning("å‘é‡è³‡æ–™åº«ç‚ºç©ºï¼Œè«‹å…ˆä¸Šå‚³æª”æ¡ˆæˆ–é¸æ“‡å…¶ä»–è³‡æ–™ä¾†æº")
        else:
            with st.spinner("æ­£åœ¨æŸ¥è©¢..."):
                try:
                    # åŸ·è¡ŒæŸ¥è©¢ï¼ˆä¸éœ€è¦å‚³éæª”æ¡ˆï¼Œå› ç‚ºå·²ç¶“åœ¨å‘é‡è³‡æ–™åº«ä¸­ï¼‰
                    results = run_rag(query, sources=data_source, files=None)
                    
                    if results and isinstance(results, list):
                        # è™•ç†æ¯å€‹çµæœ
                        for idx, result in enumerate(results):
                            if isinstance(result, tuple) and len(result) >= 2:
                                source_type = result[0]
                                answer = result[1]
                                extra_info = result[2] if len(result) > 2 else None
                                
                                # é¡¯ç¤ºä¸»è¦ç­”æ¡ˆ
                                if idx == 0:  # åªåœ¨ç¬¬ä¸€å€‹çµæœé¡¯ç¤ºæ¨™é¡Œ
                                    st.markdown("### ğŸ’¬ å›ç­”")
                                
                                # æ ¹æ“šä¾†æºé¡å‹é¡¯ç¤ºä¸åŒçš„åœ–æ¨™
                                icon = "ğŸ“„" if source_type == "docs" else "ğŸ—„ï¸"
                                st.markdown(f"{icon} **{source_type.upper()}**: {answer}")
                                
                                # é¡¯ç¤ºé¡å¤–è³‡è¨Š
                                if extra_info:
                                    with st.expander(f"æŸ¥çœ‹è©³ç´°è³‡è¨Š - {source_type}"):
                                        if source_type == "docs":
                                            st.markdown("**ç›¸é—œå…§å®¹ï¼š**")
                                            if isinstance(extra_info, list):
                                                for item in extra_info[:3]:
                                                    st.markdown(f"- {item.get('content', str(item))[:200]}...")
                                            else:
                                                st.markdown(extra_info)
                                                
                                        elif source_type == "db":
                                            st.markdown("**è³‡æ–™åº«æŸ¥è©¢çµæœï¼š**")
                                            if isinstance(extra_info, str):
                                                st.code(extra_info)
                                            else:
                                                st.json(extra_info)
                                
                                # åœ¨çµæœä¹‹é–“åŠ å…¥åˆ†éš”ç·š
                                if idx < len(results) - 1:
                                    st.divider()
                    else:
                        st.info("æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡è¨Š")
                        
                except Exception as e:
                    st.error(f"è™•ç†æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
                    st.exception(e)

# ä½¿ç”¨èªªæ˜
with st.expander("ğŸ“‹ ä½¿ç”¨èªªæ˜", expanded=False):
    st.markdown("""
    ### å‘é‡è³‡æ–™åº«æŒä¹…åŒ–åŠŸèƒ½
    
    1. **é¦–æ¬¡ä½¿ç”¨**ï¼šä¸Šå‚³æ‚¨çš„æ–‡ä»¶ï¼ˆPDFã€Wordã€Excelã€Log ç­‰ï¼‰
    2. **æª”æ¡ˆæœƒè¢«æ°¸ä¹…ç´¢å¼•**ï¼šä¸Šå‚³çš„æª”æ¡ˆæœƒè¢«è™•ç†ä¸¦å­˜å…¥å‘é‡è³‡æ–™åº«
    3. **ä¹‹å¾ŒæŸ¥è©¢**ï¼šä¸éœ€è¦é‡æ–°ä¸Šå‚³ï¼Œç›´æ¥è¼¸å…¥å•é¡Œå³å¯
    4. **æ–°å¢æª”æ¡ˆ**ï¼šéš¨æ™‚å¯ä»¥ä¸Šå‚³æ–°æª”æ¡ˆä¾†æ“´å……çŸ¥è­˜åº«
    5. **æ¸…ç©ºè³‡æ–™**ï¼šä½¿ç”¨å´é‚Šæ¬„çš„ã€Œæ¸…ç©ºå‘é‡è³‡æ–™åº«ã€æŒ‰éˆ•é‡æ–°é–‹å§‹
    
    ### æ”¯æ´çš„æª”æ¡ˆé¡å‹
    - æ–‡æª”ï¼šPDF, Word (.docx, .doc), Markdown (.md)
    - è³‡æ–™ï¼šExcel (.xlsx, .xls), CSV, JSON
    - æ—¥èªŒï¼š.log, .txt
    - ç¶²é ï¼šHTML
    
    ### æŸ¥è©¢æŠ€å·§
    - å¯ä»¥ç›´æ¥è©¢å•é—œæ–¼å·²ç´¢å¼•æª”æ¡ˆçš„ä»»ä½•å•é¡Œ
    - æ”¯æ´ä¸­æ–‡æŸ¥è©¢
    - å¯ä»¥è¦æ±‚åˆ†æã€ç¸½çµæˆ–æ¯”è¼ƒæª”æ¡ˆå…§å®¹
    """)