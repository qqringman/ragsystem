from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from config import get_config
import requests
import json
import os

class SimpleOllama:
    """å®Œå…¨ç¨ç«‹çš„ Ollama å®¢æˆ¶ç«¯ï¼Œä¸ä½¿ç”¨ä»»ä½• LangChain åŸºé¡"""
    def __init__(self, model="llama3", base_url="http://localhost:11434", temperature=0.7):
        self.model = model
        # æ™ºèƒ½è™•ç† URL - åœ¨ Docker ç’°å¢ƒä¸­è‡ªå‹•æ›¿æ› localhost
        if "localhost" in base_url and (os.path.exists("/.dockerenv") or os.getenv("DOCKER_CONTAINER")):
            base_url = base_url.replace("localhost", "ollama")
            print(f"ğŸ³ Docker ç’°å¢ƒæª¢æ¸¬ï¼šè‡ªå‹•åˆ‡æ›åˆ° {base_url}")
        self.base_url = base_url
        self.temperature = temperature
        print(f"ğŸ¤– åˆå§‹åŒ– Ollama: {model} @ {self.base_url}")
    
    def predict(self, prompt, stream=False):
        """ç›´æ¥èª¿ç”¨ Ollama HTTP API"""
        try:
            # å˜—è©¦ generate endpoint
            url = f"{self.base_url}/api/generate"
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": stream,
                "options": {
                    "temperature": self.temperature,
                    "num_predict": 2048,  # å¢åŠ æœ€å¤§è¼¸å‡ºé•·åº¦
                    "stop": ["<|im_end|>", "</s>"]  # åœæ­¢æ¨™è¨˜
                }
            }
            
            if stream:
                # æµå¼éŸ¿æ‡‰
                response = requests.post(url, json=payload, stream=True, timeout=300)
                if response.status_code == 200:
                    for line in response.iter_lines():
                        if line:
                            try:
                                data = json.loads(line)
                                if 'response' in data:
                                    yield data['response']
                                if data.get('done', False):
                                    break
                            except json.JSONDecodeError:
                                continue
                else:
                    yield f"Ollama API éŒ¯èª¤: {response.status_code}"
            else:
                # éæµå¼éŸ¿æ‡‰
                response = requests.post(url, json=payload, timeout=300)
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "")
                else:
                    # å¦‚æœ generate å¤±æ•—ï¼Œå˜—è©¦ chat endpoint
                    chat_url = f"{self.base_url}/api/chat"
                    chat_payload = {
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "stream": False,
                        "options": {"temperature": self.temperature}
                    }
                    
                    chat_response = requests.post(chat_url, json=chat_payload, timeout=300)
                    if chat_response.status_code == 200:
                        chat_result = chat_response.json()
                        return chat_result.get("message", {}).get("content", "")
                    else:
                        return f"Ollama API éŒ¯èª¤: {response.status_code} - {response.text[:200]}"
                        
        except requests.exceptions.ConnectionError as e:
            if stream:
                yield f"ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ ({self.base_url})ï¼Œè«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œã€‚éŒ¯èª¤: {str(e)}"
            else:
                return f"ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ ({self.base_url})ï¼Œè«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œã€‚éŒ¯èª¤: {str(e)}"
        except requests.exceptions.Timeout:
            if stream:
                yield "Ollama è«‹æ±‚è¶…æ™‚ï¼Œå¯èƒ½æ˜¯æ¨¡å‹è¼‰å…¥æ™‚é–“éé•·ï¼Œè«‹ç¨å¾Œå†è©¦"
            else:
                return "Ollama è«‹æ±‚è¶…æ™‚ï¼Œå¯èƒ½æ˜¯æ¨¡å‹è¼‰å…¥æ™‚é–“éé•·ï¼Œè«‹ç¨å¾Œå†è©¦"
        except Exception as e:
            if stream:
                yield f"Ollama éŒ¯èª¤: {type(e).__name__}: {str(e)}"
            else:
                return f"Ollama éŒ¯èª¤: {type(e).__name__}: {str(e)}"
    
    def __call__(self, prompt):
        """æ”¯æ´å‡½æ•¸èª¿ç”¨æ–¹å¼"""
        return self.predict(prompt)
    
    def invoke(self, prompt):
        """å…¼å®¹ LangChain çš„ invoke æ–¹æ³•"""
        return self.predict(prompt)
    
    @property
    def _llm_type(self):
        """è¿”å› LLM é¡å‹"""
        return "ollama"

def get_llm(provider=None):
    """
    ç²å– LLM å¯¦ä¾‹
    
    Args:
        provider: LLM æä¾›è€…ï¼Œå¯é¸ 'openai', 'claude', 'anthropic', 'ollama'
                 å¦‚æœä¸æŒ‡å®šï¼Œå¾ç’°å¢ƒè®Šæ•¸è®€å–
    
    Returns:
        LLM å¯¦ä¾‹
    """
    # ä½¿ç”¨é…ç½®ç³»çµ±
    provider = provider or get_config("LLM_PROVIDER")
    
    if provider in ["claude", "anthropic"]:
        api_key = get_config("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
        
        model = get_config("CLAUDE_MODEL", "claude-3-opus-20240229")
        
        return ChatAnthropic(
            model=model,
            anthropic_api_key=api_key,
            temperature=0.7,
        )
    
    elif provider == "openai":
        api_key = get_config("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        
        model = get_config("OPENAI_MODEL", "gpt-3.5-turbo")
        
        return ChatOpenAI(
            model=model,
            openai_api_key=api_key,
            temperature=0.7,
        )
    
    elif provider == "ollama":
        model = get_config("OLLAMA_MODEL", "llama3")
        base_url = get_config("OLLAMA_BASE_URL", "http://localhost:11434")
        
        # ä½¿ç”¨ç°¡å–®çš„ Ollama å¯¦ç¾ï¼Œå®Œå…¨é¿é–‹ LangChain çš„ Ollama é¡
        return SimpleOllama(
            model=model,
            base_url=base_url,
            temperature=0.7
        )
    
    else:
        raise ValueError(f"Unsupported LLM_PROVIDER: {provider}. Supported: openai, claude, anthropic, ollama")