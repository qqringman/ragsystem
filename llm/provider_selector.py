# llm/provider_selector.py
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from config import get_config
import requests
import json
import os

class SimpleOllama:
    """å®Œå…¨ç¨ç«‹çš„ Ollama å®¢æˆ¶ç«¯ï¼Œä¸ä½¿ç”¨ä»»ä½• LangChain åŸºé¡"""
    def __init__(self, model="llama3", base_url="http://localhost:11434", temperature=0.7):
        self.model = model
        # æ™ºèƒ½è™•ç† URL - åœ¨ Docker ç’°å¢ƒä¸­è‡ªå‹•æ›¿æ› localhost
        if "localhost" in base_url and (os.path.exists("/.dockerenv") or os.getenv("DOCKER_CONTAINER")):
            base_url = base_url.replace("localhost", "ollama")
            print(f"ğŸ³ Docker ç’°å¢ƒæª¢æ¸¬ï¼šè‡ªå‹•åˆ‡æ›åˆ° {base_url}")
        self.base_url = base_url
        self.temperature = temperature
        print(f"ğŸ¤– åˆå§‹åŒ– Ollama: {model} @ {self.base_url}")
    
    def predict(self, prompt, stream=False):
        """ç›´æ¥èª¿ç”¨ Ollama HTTP API - ç¢ºä¿ç¸½æ˜¯è¿”å›å­—ä¸²"""
        try:
            # å¼·åˆ¶ stream ç‚º False ä»¥é¿å…è¿”å› generator
            stream = False
            
            # å˜—è©¦ generate endpoint
            url = f"{self.base_url}/api/generate"
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,  # å¼·åˆ¶éæµå¼
                "options": {
                    "temperature": self.temperature,
                    "num_predict": 2048,
                    "stop": ["<|im_end|>", "</s>"]
                }
            }
            
            response = requests.post(url, json=payload, timeout=300)
            
            if response.status_code == 200:
                result = response.json()
                # ç¢ºä¿è¿”å›å­—ä¸²
                return str(result.get("response", ""))
            else:
                # å¦‚æœ generate å¤±æ•—ï¼Œå˜—è©¦ chat endpoint
                chat_url = f"{self.base_url}/api/chat"
                chat_payload = {
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "stream": False,
                    "options": {"temperature": self.temperature}
                }
                
                chat_response = requests.post(chat_url, json=chat_payload, timeout=300)
                if chat_response.status_code == 200:
                    chat_result = chat_response.json()
                    # ç¢ºä¿è¿”å›å­—ä¸²
                    message_content = chat_result.get("message", {}).get("content", "")
                    return str(message_content)
                else:
                    error_msg = f"Ollama API éŒ¯èª¤: {response.status_code} - {response.text[:200]}"
                    print(f"âŒ {error_msg}")
                    return error_msg
                    
        except requests.exceptions.ConnectionError as e:
            error_msg = f"ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ ({self.base_url})ï¼Œè«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œã€‚éŒ¯èª¤: {str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg
        except requests.exceptions.Timeout:
            error_msg = "Ollama è«‹æ±‚è¶…æ™‚ï¼Œå¯èƒ½æ˜¯æ¨¡å‹è¼‰å…¥æ™‚é–“éé•·ï¼Œè«‹ç¨å¾Œå†è©¦"
            print(f"âŒ {error_msg}")
            return error_msg
        except Exception as e:
            error_msg = f"Ollama éŒ¯èª¤: {type(e).__name__}: {str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg
    
    def __call__(self, prompt):
        """æ”¯æ´å‡½æ•¸èª¿ç”¨æ–¹å¼"""
        return self.predict(prompt)
    
    def invoke(self, prompt):
        """å…¼å®¹ LangChain çš„ invoke æ–¹æ³•"""
        # å¦‚æœ prompt æ˜¯ BaseMessage æˆ–å…¶ä»– LangChain ç‰©ä»¶
        if hasattr(prompt, 'content'):
            prompt = prompt.content
        elif hasattr(prompt, 'messages'):
            # å¦‚æœæ˜¯è¨Šæ¯åˆ—è¡¨
            messages = prompt.messages
            if messages:
                prompt = messages[-1].content if hasattr(messages[-1], 'content') else str(messages[-1])
            else:
                prompt = str(prompt)
        else:
            prompt = str(prompt)
        
        return self.predict(prompt)
    
    @property
    def _llm_type(self):
        """è¿”å› LLM é¡å‹"""
        return "ollama"

def get_llm(provider=None):
    """
    ç²å– LLM å¯¦ä¾‹
    
    Args:
        provider: LLM æä¾›è€…ï¼Œå¯é¸ 'openai', 'claude', 'anthropic', 'ollama'
                 å¦‚æœä¸æŒ‡å®šï¼Œå¾ç’°å¢ƒè®Šæ•¸è®€å–
    
    Returns:
        LLM å¯¦ä¾‹
    """
    # ä½¿ç”¨é…ç½®ç³»çµ±
    provider = provider or get_config("LLM_PROVIDER")
    
    if provider in ["claude", "anthropic"]:
        api_key = get_config("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
        
        model = get_config("CLAUDE_MODEL", "claude-3-opus-20240229")
        
        return ChatAnthropic(
            model=model,
            anthropic_api_key=api_key,
            temperature=0.7,
        )
    
    elif provider == "openai":
        api_key = get_config("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        
        model = get_config("OPENAI_MODEL", "gpt-3.5-turbo")
        
        return ChatOpenAI(
            model=model,
            openai_api_key=api_key,
            temperature=0.7,
        )
    
    elif provider == "ollama":
        model = get_config("OLLAMA_MODEL", "llama3")
        base_url = get_config("OLLAMA_BASE_URL", "http://localhost:11434")
        
        # ä½¿ç”¨ç°¡å–®çš„ Ollama å¯¦ç¾ï¼Œå®Œå…¨é¿é–‹ LangChain çš„ Ollama é¡
        return SimpleOllama(
            model=model,
            base_url=base_url,
            temperature=0.7
        )
    
    else:
        raise ValueError(f"Unsupported LLM_PROVIDER: {provider}. Supported: openai, claude, anthropic, ollama")