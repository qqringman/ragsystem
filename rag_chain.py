# -*- coding: utf-8 -*-
import os
import shutil
from loader.doc_parser import load_and_split_documents
from vectorstore.index_manager import get_vectorstore
from llm.provider_selector import get_llm
from utils.highlighter import highlight_chunks
from db.sql_executor import query_database
from config import get_config
import tempfile
from typing import List, Dict, Any, Optional, Tuple
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import HumanMessage, AIMessage


class RAGChain:
    """å¢å¼·çš„ RAG éˆï¼Œæ”¯æ´å°è©±è¨˜æ†¶å’Œå¤šè³‡æ–™ä¾†æº"""
    
    def __init__(self, memory_window: int = 10):
        """
        åˆå§‹åŒ– RAG éˆ
        
        Args:
            memory_window: å°è©±è¨˜æ†¶è¦–çª—å¤§å°
        """
        self.memory = ConversationBufferWindowMemory(
            k=memory_window,
            return_messages=True
        )
        self.llm = None
        self._init_llm()
    
    def _init_llm(self):
        """åˆå§‹åŒ– LLM"""
        llm_provider = get_config("LLM_PROVIDER")
        self.llm = get_llm(provider=llm_provider)
    
    def add_to_memory(self, human_input: str, ai_output: str):
        """æ·»åŠ å°è©±åˆ°è¨˜æ†¶"""
        self.memory.chat_memory.add_user_message(human_input)
        self.memory.chat_memory.add_ai_message(ai_output)
    
    def get_conversation_context(self) -> str:
        """ç²å–å°è©±ä¸Šä¸‹æ–‡"""
        messages = self.memory.chat_memory.messages
        if not messages:
            return ""
        
        context = "ä¹‹å‰çš„å°è©±è¨˜éŒ„ï¼š\n"
        for msg in messages:
            if isinstance(msg, HumanMessage):
                context += f"ç”¨æˆ¶ï¼š{msg.content}\n"
            elif isinstance(msg, AIMessage):
                context += f"åŠ©æ‰‹ï¼š{msg.content}\n"
        
        return context
    
    def run_query(self, query: str, sources: List[str], files: Optional[List[str]] = None) -> List[Tuple[str, str, Any]]:
        """
        åŸ·è¡ŒæŸ¥è©¢
        
        Args:
            query: ä½¿ç”¨è€…æŸ¥è©¢
            sources: è³‡æ–™ä¾†æºåˆ—è¡¨
            files: æª”æ¡ˆåˆ—è¡¨ï¼ˆå¯é¸ï¼‰
            
        Returns:
            çµæœåˆ—è¡¨ï¼Œæ¯å€‹çµæœæ˜¯ (ä¾†æºé¡å‹, ç­”æ¡ˆ, é¡å¤–è³‡è¨Š) çš„å…ƒçµ„
        """
        results = []
        
        # ç²å–å°è©±ä¸Šä¸‹æ–‡
        context = self.get_conversation_context()
        
        # å¢å¼·æŸ¥è©¢ï¼ˆåŠ å…¥å°è©±ä¸Šä¸‹æ–‡ï¼‰
        if context:
            enhanced_query = f"{context}\n\nç•¶å‰å•é¡Œï¼š{query}"
        else:
            enhanced_query = query
        
        if "docs" in sources:
            doc_results = self._query_documents(enhanced_query, files)
            if doc_results:
                results.extend(doc_results)
        
        if "db" in sources:
            db_results = self._query_database(query)  # è³‡æ–™åº«æŸ¥è©¢ä½¿ç”¨åŸå§‹æŸ¥è©¢
            if db_results:
                results.extend(db_results)
        
        # æœªä¾†å¯ä»¥æ“´å……å…¶ä»–è³‡æ–™ä¾†æº
        # if "web" in sources:
        #     web_results = self._query_web(query)
        #     if web_results:
        #         results.extend(web_results)
        
        return results
    
    def _query_documents(self, query: str, files: Optional[List[str]] = None) -> List[Tuple[str, str, Any]]:
        """æŸ¥è©¢æ–‡ä»¶"""
        try:
            # åˆ¤æ–·æ˜¯è‡¨æ™‚åˆ†æé‚„æ˜¯çŸ¥è­˜åº«æŸ¥è©¢
            if files:
                # è‡¨æ™‚æª”æ¡ˆåˆ†ææ¨¡å¼
                print(f"ğŸ“Š è‡¨æ™‚åˆ†ææ¨¡å¼ï¼šè™•ç† {len(files)} å€‹æª”æ¡ˆ")
                
                # å‰µå»ºè‡¨æ™‚å‘é‡è³‡æ–™åº«
                temp_dir = tempfile.mkdtemp()
                temp_vectorstore_path = os.path.join(temp_dir, "temp_chroma")
                
                try:
                    # è¼‰å…¥æ–‡æª”
                    docs = load_and_split_documents(files)
                    if not docs:
                        return [("docs", "ç„¡æ³•è¼‰å…¥æª”æ¡ˆå…§å®¹", None)]
                    
                    # å‰µå»ºè‡¨æ™‚å‘é‡è³‡æ–™åº«
                    from langchain_community.vectorstores import Chroma
                    from vectorstore.index_manager import get_embeddings
                    
                    embeddings = get_embeddings()
                    temp_vs = Chroma(
                        embedding_function=embeddings,
                        persist_directory=temp_vectorstore_path
                    )
                    
                    # æ·»åŠ æ–‡æª”åˆ°è‡¨æ™‚è³‡æ–™åº«
                    temp_vs.add_documents(docs)
                    print(f"âœ… å·²å°‡ {len(docs)} å€‹æ–‡æª”ç‰‡æ®µåŠ å…¥è‡¨æ™‚è³‡æ–™åº«")
                    
                    # ä½¿ç”¨è‡¨æ™‚è³‡æ–™åº«é€²è¡ŒæŸ¥è©¢
                    search_k = int(get_config("SEARCH_K", "5"))
                    rel_docs = temp_vs.similarity_search(query, k=search_k)
                    
                finally:
                    # æ¸…ç†è‡¨æ™‚å‘é‡è³‡æ–™åº«
                    if os.path.exists(temp_dir):
                        shutil.rmtree(temp_dir)
                        print("ğŸ§¹ å·²æ¸…ç†è‡¨æ™‚å‘é‡è³‡æ–™åº«")
                        
            else:
                # çŸ¥è­˜åº«æŸ¥è©¢æ¨¡å¼
                print("ğŸ“š çŸ¥è­˜åº«æŸ¥è©¢æ¨¡å¼")
                
                # ç²å–æŒä¹…åŒ–å‘é‡è³‡æ–™åº«
                vs = get_vectorstore()
                
                # å¾å‘é‡è³‡æ–™åº«æœå°‹
                search_k = int(get_config("SEARCH_K", "5"))
                
                try:
                    rel_docs = vs.similarity_search(query, k=search_k)
                except Exception as e:
                    if "collection" in str(e).lower() and "does not exist" in str(e).lower():
                        return [("docs", "çŸ¥è­˜åº«ç‚ºç©ºï¼Œè«‹å…ˆå»ºç«‹çŸ¥è­˜åº«", None)]
                    else:
                        raise e
            
            # è™•ç†æŸ¥è©¢çµæœ
            if not rel_docs:
                if files:
                    return [("docs", "åœ¨ä¸Šå‚³çš„æª”æ¡ˆä¸­æ‰¾ä¸åˆ°ç›¸é—œå…§å®¹", None)]
                else:
                    return [("docs", "åœ¨çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œå…§å®¹", None)]
            
            print(f"ğŸ” æ‰¾åˆ° {len(rel_docs)} å€‹ç›¸é—œæ–‡æª”ç‰‡æ®µ")
            
            # æ§‹å»ºä¸Šä¸‹æ–‡
            context = "\n\n---\n\n".join([
                f"æ–‡æª” {i+1}:\n{doc.page_content}" 
                for i, doc in enumerate(rel_docs)
            ])
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦ç‰¹æ®Šè™•ç†
            is_log_analysis = any(
                doc.metadata.get('file_type') == 'log' or 
                doc.metadata.get('log_type') is not None 
                for doc in rel_docs
            )
            
            # æ§‹å»ºæç¤º
            if is_log_analysis:
                prompt = self._build_log_analysis_prompt(query, context, rel_docs)
            else:
                prompt = self._build_general_prompt(query, context)
            
                # ç”Ÿæˆç­”æ¡ˆ
                try:
                    raw_answer = self.llm.predict(prompt)
                    
                    # ç¢ºä¿ç­”æ¡ˆæ˜¯å­—ä¸²
                    if isinstance(raw_answer, str):
                        answer = raw_answer
                    elif hasattr(raw_answer, '__iter__') and not isinstance(raw_answer, str):
                        # å¦‚æœæ˜¯ generator æˆ–å…¶ä»–å¯è¿­ä»£ç‰©ä»¶
                        answer = ''.join(str(chunk) for chunk in raw_answer)
                    else:
                        # å…¶ä»–æƒ…æ³ï¼Œè½‰æ›ç‚ºå­—ä¸²
                        answer = str(raw_answer)
                    
                    # æº–å‚™ç›¸é—œæ–‡æª”çš„å…ƒæ•¸æ“š
                    highlighted = self._prepare_highlights(rel_docs, is_log_analysis)
                    
                    return [("docs", answer, highlighted)]
                    
                except Exception as e:
                    print(f"âŒ LLM ç”Ÿæˆç­”æ¡ˆéŒ¯èª¤ï¼š{str(e)}")
                    return [("docs", f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}", None)]
            
            # æº–å‚™ç›¸é—œæ–‡æª”çš„å…ƒæ•¸æ“š
            highlighted = self._prepare_highlights(rel_docs, is_log_analysis)
            
            return [("docs", answer, highlighted)]
            
        except Exception as e:
            print(f"âŒ æ–‡æª”æŸ¥è©¢éŒ¯èª¤ï¼š{str(e)}")
            return [("docs", f"æŸ¥è©¢å¤±æ•—ï¼š{str(e)}", None)]
    
    def _query_database(self, query: str) -> List[Tuple[str, str, Any]]:
        """æŸ¥è©¢è³‡æ–™åº«"""
        try:
            sql_result = query_database(query)
            return [("db", f"æŸ¥è©¢çµæœï¼š{sql_result}", sql_result)]
        except Exception as e:
            return [("db", f"è³‡æ–™åº«æŸ¥è©¢å¤±æ•—ï¼š{str(e)}", None)]
    
    def _build_general_prompt(self, query: str, context: str) -> str:
        """æ§‹å»ºä¸€èˆ¬æŸ¥è©¢æç¤º"""
        return f"""ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šæä¾›çš„æ–‡æª”å…§å®¹å›ç­”å•é¡Œã€‚

ç›¸é—œæ–‡æª”å…§å®¹ï¼š
{context}

å•é¡Œï¼š{query}

è«‹æä¾›æº–ç¢ºã€ç›¸é—œçš„ç­”æ¡ˆã€‚å¦‚æœå…§å®¹ä¸­åŒ…å«å…·é«”çš„æ•¸æ“šæˆ–äº‹å¯¦ï¼Œè«‹å¼•ç”¨å®ƒå€‘ã€‚
å¦‚æœæ–‡æª”ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚"""
    
    def _build_log_analysis_prompt(self, query: str, context: str, docs: List) -> str:
        """æ§‹å»º Log åˆ†ææç¤º"""
        log_types = set(doc.metadata.get('log_type', 'general') for doc in docs if doc.metadata.get('log_type'))
        
        if 'android_anr' in log_types:
            return f"""ä½ æ˜¯ Android ANR åˆ†æå°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹ ANR trace å…§å®¹å›ç­”å•é¡Œï¼š

å•é¡Œï¼š{query}

ANR Trace å…§å®¹ï¼š
{context}

è«‹æä¾›ï¼š
1. ANR çš„ç›´æ¥åŸå› 
2. å—å½±éŸ¿çš„ç·šç¨‹ï¼ˆç‰¹åˆ¥æ˜¯ä¸»ç·šç¨‹ï¼‰
3. å¯èƒ½çš„å„ªåŒ–å»ºè­°
4. å¦‚æœæœ‰æ­»é–é¢¨éšªï¼Œè«‹ç‰¹åˆ¥æŒ‡å‡º"""
        
        elif 'android_tombstone' in log_types:
            return f"""ä½ æ˜¯ Android å´©æ½°åˆ†æå°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹ tombstone å…§å®¹å›ç­”å•é¡Œï¼š

å•é¡Œï¼š{query}

Tombstone å…§å®¹ï¼š
{context}

è«‹æä¾›ï¼š
1. å´©æ½°çš„æ ¹æœ¬åŸå› 
2. å´©æ½°é¡å‹ï¼ˆå¦‚ç©ºæŒ‡é‡ã€è¨˜æ†¶é«”éŒ¯èª¤ç­‰ï¼‰
3. é—œéµçš„ backtrace è§£é‡‹
4. ä¿®å¾©å»ºè­°"""
        
        else:
            return f"""ä½ æ˜¯ Log åˆ†æå°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹ log å…§å®¹å›ç­”å•é¡Œï¼š

å•é¡Œï¼š{query}

Log å…§å®¹ï¼š
{context}

è«‹æä¾›è©³ç´°çš„åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. å•é¡Œçš„ç›´æ¥ç­”æ¡ˆ
2. ç›¸é—œçš„éŒ¯èª¤æˆ–ç•°å¸¸
3. å¯èƒ½çš„åŸå› åˆ†æ
4. å»ºè­°çš„è§£æ±ºæ–¹æ¡ˆ"""
    
    def _prepare_highlights(self, docs: List, is_log_analysis: bool) -> List[Dict[str, Any]]:
        """æº–å‚™é«˜äº®é¡¯ç¤ºçš„æ–‡æª”"""
        highlighted = []
        
        for doc in docs:
            doc_info = {
                'content': doc.page_content,
                'source': doc.metadata.get('source', ''),
                'score': doc.metadata.get('score', 0)
            }
            
            # æ·»åŠ ç‰¹æ®Šçš„ log å…ƒæ•¸æ“š
            if is_log_analysis:
                if doc.metadata.get('chunk_type'):
                    doc_info['chunk_type'] = doc.metadata['chunk_type']
                if doc.metadata.get('severity'):
                    doc_info['severity'] = doc.metadata['severity']
                if doc.metadata.get('error_count'):
                    doc_info['error_count'] = doc.metadata['error_count']
                if doc.metadata.get('crash_type'):
                    doc_info['crash_type'] = doc.metadata['crash_type']
            
            highlighted.append(doc_info)
        
        return highlighted


# å…¨å±€ RAG å¯¦ä¾‹
_rag_chain = None


def get_rag_chain() -> RAGChain:
    """ç²å– RAG éˆå¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _rag_chain
    if _rag_chain is None:
        _rag_chain = RAGChain()
    return _rag_chain


def run_rag(query: str, sources: List[str], files: Optional[List[str]] = None) -> List[Tuple[str, str, Any]]:
    """
    åŸ·è¡Œ RAG æŸ¥è©¢
    
    Args:
        query: æŸ¥è©¢å•é¡Œ
        sources: è³‡æ–™ä¾†æºåˆ—è¡¨ ['docs', 'db', 'web', ...]
        files: æ–°æª”æ¡ˆåˆ—è¡¨ï¼ˆå¯é¸ï¼‰
        
    Returns:
        çµæœåˆ—è¡¨ï¼Œæ¯å€‹çµæœæ˜¯ (ä¾†æºé¡å‹, ç­”æ¡ˆ, é¡å¤–è³‡è¨Š) çš„å…ƒçµ„
    """
    rag_chain = get_rag_chain()
    results = rag_chain.run_query(query, sources, files)
    
    # å¦‚æœæœ‰æˆåŠŸçš„çµæœï¼Œæ›´æ–°å°è©±è¨˜æ†¶
    if results:
        combined_answer = ""
        for result in results:
            if isinstance(result, tuple) and len(result) >= 2:
                source_type = result[0]
                answer = result[1]
                if answer and not answer.startswith("æŸ¥è©¢å¤±æ•—"):
                    if combined_answer:
                        combined_answer += f"\n\n[ä¾†æº: {source_type.upper()}]\n"
                    combined_answer += answer
        
        if combined_answer:
            rag_chain.add_to_memory(query, combined_answer)
    
    return results


def run_query(query: str, sources: List[str], files: Optional[List[str]] = None) -> List[Tuple[str, str, Any]]:
    """å‘å¾Œå…¼å®¹çš„åˆ¥å"""
    return run_rag(query, sources, files)